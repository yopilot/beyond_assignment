#!/usr/bin/env python3
"""
Unified setup and test script for safetensors-only model loading.
Performs environment setup, checks safetensors support, verifies model caching,
loads model, and tests generation using safetensors format.
"""

import os
import sys
import subprocess
import logging
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Logging Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Env Vars â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def set_environment_variables():
    os.environ['TRANSFORMERS_PREFER_SAFETENSORS'] = '1'
    os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'
    logger.info("âœ… Environment variables set for safetensors-only")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Install Requirements â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def install_requirements():
    try:
        logger.info("ğŸ“¦ Installing requirements...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-r", "requirements.txt", "--upgrade"])
        logger.info("âœ… Requirements installed successfully")
    except subprocess.CalledProcessError as e:
        logger.error(f"âŒ Failed to install requirements: {e}")
        sys.exit(1)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Safetensors Verification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def verify_safetensors():
    try:
        import safetensors
        import transformers
        logger.info(f"âœ… safetensors version: {safetensors.__version__}")
        logger.info(f"âœ… transformers version: {transformers.__version__}")
    except ImportError as e:
        logger.error(f"âŒ Required module not found: {e}")
        sys.exit(1)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Cache Check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def verify_cache(model_name="DialoGPT-medium"):
    cache_dir = Path.home() / ".cache" / "huggingface" / "transformers"
    if not cache_dir.exists():
        logger.info("â„¹ï¸ No cache directory found.")
        return False

    model_dirs = list(cache_dir.glob(f"**/*{model_name.replace('/', '_')}*"))
    found_safetensors, found_bin = False, False

    for model_dir in model_dirs:
        safetensors_files = list(model_dir.glob("*.safetensors"))
        bin_files = list(model_dir.glob("pytorch_model*.bin"))

        if safetensors_files:
            found_safetensors = True
        if bin_files:
            found_bin = True

        logger.info(f"ğŸ—‚ï¸ Cache: {model_dir.name} - Safetensors: {len(safetensors_files)}, Bin: {len(bin_files)}")

    if found_safetensors and not found_bin:
        logger.info("âœ… Cache OK: Only safetensors found")
    elif found_bin:
        logger.warning("âš ï¸ PyTorch bin files also found. Setup may not be safetensors-only.")
    else:
        logger.info("ğŸ”„ Model not yet cached")
    
    return found_safetensors

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Model Loading Test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def test_model_loading():
    try:
        model_name = "microsoft/DialoGPT-medium"
        logger.info(f"ğŸ§  Loading model: {model_name}")

        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=False)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            use_safetensors=True,
            trust_remote_code=False,
            low_cpu_mem_usage=True,
            torch_dtype=torch.float32
        )

        generator = pipeline(
            'text-generation',
            model=model,
            tokenizer=tokenizer,
            device=-1
        )

        output = generator("Hello, how are you?", max_new_tokens=10, do_sample=False)
        logger.info(f"âœ… Test output: {output[0]['generated_text']}")
        return True

    except Exception as e:
        logger.error(f"âŒ Model loading failed: {e}")
        return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main Entry â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def main():
    logger.info("ğŸš€ Starting safetensors-only setup & test")

    set_environment_variables()
    install_requirements()
    verify_safetensors()

    logger.info("ğŸ” Checking existing cache...")
    already_cached = verify_cache()

    logger.info("ğŸ§ª Testing model loading...")
    if test_model_loading():
        logger.info("ğŸ‰ All tests passed! Safetensors-only setup is working.")
        if not already_cached:
            logger.info("ğŸ” Verifying updated cache after model load...")
            verify_cache()
    else:
        logger.error("âŒ Safetensors test failed. Check above logs for issues.")
        sys.exit(1)

if __name__ == "__main__":
    main()
